{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Text Classifier\n",
    "\n",
    "## Overview\n",
    "\n",
    "Naive Bayes is a probabilistic classification algorithm based on Bayes' Theorem, commonly used in Natural Language Processing (NLP) tasks such as spam detection, sentiment analysis, and document classification. It assumes that features (words in our case) are conditionally independent given the class label. This \"naive\" assumption of independence simplifies calculations and makes the model computationally efficient, even for large datasets.\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "Bayes' Theorem provides a way to calculate the posterior probability of a class given observed data. The formula for Bayes' Theorem is:\n",
    " \n",
    "#### P(C|X) = P(C) * Prod(P(Xi|C)) /P(X) ; i = 1 to n(number of samples)\n",
    " \n",
    "- P(C|X) is the conditional probability of X being in class C \n",
    "- P(C) is the prior probability of each class c\n",
    "- P(Xi|C) is the probability of the ith example being in class C\n",
    "\n",
    "### Dataset used:\n",
    "Stanford NLP's IMDB dataset:\n",
    "`Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).`\n",
    "\n",
    "### Steps\n",
    "1. **Preprocessing**: Tokenize text and build a vocabulary of unique words.\n",
    "2. **Feature Representation**: Represent each document as a Bag of Words (BoW) vector.\n",
    "3. **Model Training**: Implement Naive Bayes to estimate the probabilities for each class and each word given a class.\n",
    "4. **Prediction and Evaluation**: Use the model to classify new documents and evaluate its performance.\n",
    "5. **Comparison**: Compare the custom implementation with `scikit-learn`â€™s `MultinomialNB` to observe differences in efficiency and accuracy.\n",
    "\n",
    "### Discussion\n",
    "\n",
    "Naive Bayes classifiers are popular for text classification because of their simplicity and efficiency, especially with BoW data. While the independence assumption is rarely true for natural language data, Naive Bayes still performs well in practice. This project demonstrates how the model's assumptions and mechanics work under the hood, and it provides insight into how simple probabilistic methods can be effective for text classification.\n",
    "\n",
    "---\n",
    "\n",
    "Through this project, we gain practical experience with both the fundamentals of the Naive Bayes algorithm and text classification workflows. By implementing and comparing with a pre-built library, we gain a deeper understanding of model assumptions, optimizations, and real-world application of machine learning in NLP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krish\\miniconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = load_dataset(\"imdb\") # loading the dataset\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the required data and shuffling splitting the train set itself as train and test\n",
    "train_X = imdb['train']['text']\n",
    "train_Y = imdb['train']['label']\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(\n",
    "    train_X, train_Y, train_size=20000, shuffle=True\n",
    ") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = test_X[:100]\n",
    "test_Y = test_Y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999\r"
     ]
    }
   ],
   "source": [
    "# This function normalizes the input data then tokenizes and removes stop words to reduce noise in dataset.\n",
    "def preprocess(texts):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    vocab = set()\n",
    "    processed_texts = []\n",
    "\n",
    "    for i, text in enumerate(texts):\n",
    "        print(i, end='\\r')\n",
    "        words = [\n",
    "            word for word in nltk.word_tokenize(text.lower()) if word not in stop_words\n",
    "        ]\n",
    "        vocab.update(words)  # Adds words to vocab\n",
    "        processed_texts.append(\" \".join(words))\n",
    "\n",
    "    return vocab, processed_texts\n",
    "vocab, processed_texts = preprocess(train_X)\n",
    "vocab, processed_texts_test = preprocess(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vocab)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Found it efficient use the existing function than to implement it as the vocab size is quiet big\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=vocab)\n",
    "bows = vectorizer.transform(processed_texts).toarray()\n",
    "bows_test = vectorizer.transform(processed_texts_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(bows)\n",
    "Y = np.array(train_Y)\n",
    "X_test = np.array(bows_test)\n",
    "Y_test = np.array(test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class implements Naive Bayes\n",
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.class_prior = {} # to store P(C) for every C\n",
    "        self.likelihood = defaultdict(lambda: defaultdict(lambda: 0)) # to store P(X|C), for every X and C\n",
    "        self.classes = [] # store unique classes\n",
    "        self.class_word_counts = defaultdict(int) # How many words of the vocab lies in this class\n",
    "        self.vocab_size = 0\n",
    "\n",
    "    def train(self, X, Y):\n",
    "        self.classes = np.unique(Y) # Unique labels are the classes\n",
    "        self.vocab_size = X.shape[1] # X is of shape number of samples x vocab_size (so using that or can use vocab_size from above as well)\n",
    "\n",
    "        for c in self.classes:\n",
    "            self.class_prior[c] = np.mean(Y == c) # Divides number of y == c with len of y (probability of occurunce of this class in the dataset)\n",
    "\n",
    "        for c in self.classes:\n",
    "            class_sample = X[Y == c] # Samples that has y=c as labels\n",
    "            self.class_word_counts[c] = np.sum(class_sample) # number of words in class c\n",
    "\n",
    "            for word_idx in range(self.vocab_size):\n",
    "                self.likelihood[c][word_idx] = (\n",
    "                    np.sum(class_sample[:, word_idx]) + 1\n",
    "                )  # +1 is added to avoid zero probabilities of words that don't occur in a class. np.sum(class_sample[:word_idx]) gives the number of samples in class C that has the word in word_idx\n",
    "\n",
    "            self.likelihood[c] = {\n",
    "                word_idx: self.likelihood[c][word_idx]\n",
    "                / (self.class_word_counts[c] + self.vocab_size)\n",
    "                for word_idx in self.likelihood[c]\n",
    "            }  # We are normalising the probability with the number of words in class C and to avoid division by zero, we are adding self.vocab_size\n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_probs = {}\n",
    "            for c in self.class_prior:\n",
    "                log_prob = np.log(self.class_prior[c]) #using log to avoid overflowing\n",
    "\n",
    "                for word_idx in range(len(x)):\n",
    "                    if (\n",
    "                        x[word_idx] > 0\n",
    "                    ):  \n",
    "                        log_prob += np.log(self.likelihood[c][word_idx]) # Adding instead of multipluying as we have taken log\n",
    "\n",
    "                class_probs[c] = log_prob\n",
    "\n",
    "            predictions.append(max(class_probs, key=class_probs.get))\n",
    "\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = NaiveBayes()\n",
    "nb_classifier.train(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of implemented Naive Bayes Classifier:  0.88\n"
     ]
    }
   ],
   "source": [
    "predictions = nb_classifier.predict(X_test)\n",
    "accuracy = np.mean(predictions == Y_test)\n",
    "print(\"Accuracy of implemented Naive Bayes Classifier: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "\n",
    "nb_model.fit(X, Y)\n",
    "\n",
    "test_predictions = nb_model.predict(X_test)\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(Y_test, test_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\r"
     ]
    }
   ],
   "source": [
    "# custom input\n",
    "text = [\"This was a very interseting movie!! I had tears in my eyes at the end but this movie will be very close to me!\", \"Oh man the movie is just my money going to water:(\"]\n",
    "_, processed = preprocess(text)\n",
    "vectorized_text = np.array(vectorizer.transform(processed).toarray())\n",
    "prediction = nb_classifier.predict(vectorized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4710"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
