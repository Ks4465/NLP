{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Text Representation and Vectorization\n",
    "\n",
    "In NLP, text representation is essential to convert human language into a format that machine learning models can process. Here are some of the most common text representation techniques:\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "- **Bag of Words** is a basic representation technique where each word in the text is treated as a separate feature, and the occurrence of each word is counted.\n",
    "- In BoW, we ignore word order and simply focus on word frequency.\n",
    "- Example: For the sentences \"I love NLP\" and \"NLP is fun,\" the vocabulary would be `[\"I\", \"love\", \"NLP\", \"is\", \"fun\"]`, with each sentence represented as a vector of word counts.\n",
    "  \n",
    "### 2. TF-IDF (Term Frequency-Inverse Document Frequency)\n",
    "- **TF-IDF** is a statistical measure that reflects the importance of a word in a document relative to a larger corpus.\n",
    "- It is calculated as the product of **Term Frequency (TF)** (the number of occurrences of a word in a document) and **Inverse Document Frequency (IDF)** (which reduces the weight of words that appear frequently across many documents).\n",
    "- Formula:\n",
    "  \\[\n",
    "  \\text{TF-IDF} = \\text{TF} \\times \\log\\left(\\frac{\\text{Total Documents}}{\\text{Number of Documents containing the Term}}\\right)\n",
    "  \\]\n",
    "- TF-IDF reduces the weight of common words (like \"is\" or \"the\") that are less informative and boosts the weight of rare, informative words.\n",
    "\n",
    "### 3. Word Embeddings\n",
    "- **Word Embeddings** are dense vector representations of words that capture semantic relationships between them. \n",
    "- Common methods include **Word2Vec** and **GloVe**:\n",
    "  - **Word2Vec**: Learns word representations using two architectures:\n",
    "    - **CBOW (Continuous Bag of Words)**: Predicts a center word from context words.\n",
    "    - **Skip-gram**: Predicts context words given a center word.\n",
    "  - **GloVe**: A model that learns word vectors by analyzing global word co-occurrence statistics from a large corpus.\n",
    "- Word embeddings allow words with similar meanings to have similar vector representations.\n",
    "\n",
    "### 4. Sentence and Document Embeddings\n",
    "- For longer texts, we need representations beyond single-word vectors to capture the context of entire sentences or documents.\n",
    "- **Sentence Embeddings**: Represent the meaning of entire sentences, with popular methods like Universal Sentence Encoder (USE) and Sentence-BERT.\n",
    "- **Document Embeddings**: Capture document-level semantics by aggregating word embeddings or using models like Doc2Vec.\n",
    "\n",
    "### Comparison of Text Representation Techniques\n",
    "\n",
    "| Technique     | Purpose                                | Pros                          | Cons                           |\n",
    "|---------------|----------------------------------------|-------------------------------|--------------------------------|\n",
    "| Bag of Words  | Simple word frequency representation   | Simple, easy to implement     | Ignores word order and context |\n",
    "| TF-IDF        | Weighted frequency representation      | Emphasizes unique words       | Still sparse, lacks context    |\n",
    "| Word Embeddings (e.g., Word2Vec, GloVe) | Dense word representation | Captures meaning and context  | Context-free embeddings        |\n",
    "| Sentence Embeddings (e.g., USE, Sentence-BERT) | Dense sentence/document representation | Contextual, advanced | Computationally intensive |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import gensim.downloader as api\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"I love NLP\", \"NLP is fun\", \"I love learning about NLP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   about  fun  is  learning  love  nlp\n",
      "0      0    0   0         0     1    1\n",
      "1      0    1   1         0     0    1\n",
      "2      1    0   0         1     1    1\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "bow_matrix = vectorizer.fit_transform(documents)\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      about       fun        is  learning      love       nlp\n",
      "0  0.000000  0.000000  0.000000  0.000000  0.789807  0.613356\n",
      "1  0.000000  0.652491  0.652491  0.000000  0.000000  0.385372\n",
      "2  0.584483  0.000000  0.000000  0.584483  0.444514  0.345205\n"
     ]
    }
   ],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out()\n",
    ")\n",
    "print(tfidf_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6721    -0.17858    0.20188    0.63581   -0.31304    1.2183\n",
      " -0.13314   -1.1776    -0.27009    0.52236   -0.0086308 -0.056211\n",
      "  1.3483    -1.0131    -1.0985    -0.24086   -0.0066808 -0.14822\n",
      " -0.044672   0.54472   -0.92966   -0.69065    0.91675    0.054691\n",
      " -0.2081     1.1201     0.92071   -1.2295     0.107      0.65846\n",
      " -0.84775   -0.14577   -0.69941    0.83514    0.90995   -0.70647\n",
      " -0.78513    0.82611    1.0785     0.29806    1.0306     0.19589\n",
      " -0.5562     0.43684    0.5979     0.77427    0.40238    0.57069\n",
      "  0.29321    1.0723   ]\n",
      "Similarity between NLP and AI: -0.08673771\n"
     ]
    }
   ],
   "source": [
    "glove_vectors = api.load(\"glove-wiki-gigaword-50\")\n",
    "word_vector = glove_vectors[\"nlp\"] # lowercase text are only available!\n",
    "print(word_vector)\n",
    "similarity = glove_vectors.similarity(\"nlp\", \"ai\") \n",
    "print(\"Similarity between NLP and AI:\", similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.41736597  0.90427005 -1.0050299  -0.06202102  0.49725997  0.80667007\n",
      " -0.14855     0.80365    -0.15653998 -0.66973996  0.23435399  0.62476\n",
      "  0.925871   -0.97099996  0.92566     0.89915    -1.54596    -0.52625\n",
      "  0.13695401  0.66199005  0.4871601   0.37035    -0.214214    0.10100996\n",
      "  0.71358    -2.0874999  -1.1362001  -1.1496099  -0.53599     0.27389997\n",
      "  1.6723      0.02930999 -0.77656007  0.46056286  0.34866    -0.05741701\n",
      "  0.19444    -0.207748   -0.73038995 -0.10751998  0.235544    0.96423995\n",
      " -0.46993998 -0.48727497 -0.25399995  0.4621299  -0.66081    -1.9451499\n",
      " -0.68797004 -0.49784005]\n",
      "\n",
      "\n",
      " [ 0.37854    1.8233    -1.2648    -0.1043     0.35829    0.60029\n",
      " -0.17538    0.83767   -0.056798  -0.75795    0.22681    0.98587\n",
      "  0.60587   -0.31419    0.28877    0.56013   -0.77456    0.071421\n",
      " -0.5741     0.21342    0.57674    0.3868    -0.12574    0.28012\n",
      "  0.28135   -1.8053    -1.0421    -0.19255   -0.55375   -0.054526\n",
      "  1.5574     0.39296   -0.2475     0.34251    0.45365    0.16237\n",
      "  0.52464   -0.070272  -0.83744   -1.0326     0.45946    0.25302\n",
      " -0.17837   -0.73398   -0.20025    0.2347    -0.56095   -2.2839\n",
      "  0.0092753 -0.60284  ]\n",
      "[('king', 0.8859834671020508), ('queen', 0.8609582185745239)]\n"
     ]
    }
   ],
   "source": [
    "king = glove_vectors[\"king\"]\n",
    "man = glove_vectors[\"man\"]\n",
    "woman = glove_vectors[\"woman\"]\n",
    "queen = glove_vectors[\"queen\"]\n",
    "print(king-man+woman)\n",
    "print(\"\\n\\n\", queen)\n",
    "most_similar = glove_vectors.similar_by_vector(king-man+woman, topn=2)\n",
    "print(most_similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"I love natural language processing\",\n",
    "    \"Natural language processing is amazing\",\n",
    "    \"Word embeddings are useful for NLP tasks\",\n",
    "    \"I am learning about Word2Vec and GloVe\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(\n",
    "    sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4\n",
    ")\n",
    "word2vec_model.save(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"../models/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv[\"natural\"]\n",
    "similar_words = model.wv.most_similar(\"natural\", topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'natural': [-8.6196875e-03  3.6657380e-03  5.1898835e-03  5.7419385e-03\n",
      "  7.4669183e-03 -6.1676754e-03  1.1056137e-03  6.0472824e-03\n",
      " -2.8400505e-03 -6.1735227e-03 -4.1022300e-04 -8.3689485e-03\n",
      " -5.6000124e-03  7.1045388e-03  3.3525396e-03  7.2256695e-03\n",
      "  6.8002474e-03  7.5307419e-03 -3.7891543e-03 -5.6180597e-04\n",
      "  2.3483764e-03 -4.5190323e-03  8.3887316e-03 -9.8581640e-03\n",
      "  6.7646410e-03  2.9144168e-03 -4.9328315e-03  4.3981876e-03\n",
      " -1.7395747e-03  6.7113843e-03  9.9648498e-03 -4.3624435e-03\n",
      " -5.9933780e-04 -5.6956373e-03  3.8508223e-03  2.7866268e-03\n",
      "  6.8910765e-03  6.1010956e-03  9.5384968e-03  9.2734173e-03\n",
      "  7.8980681e-03 -6.9895042e-03 -9.1558648e-03 -3.5575271e-04\n",
      " -3.0998408e-03  7.8943167e-03  5.9385742e-03 -1.5456629e-03\n",
      "  1.5109634e-03  1.7900408e-03  7.8175711e-03 -9.5101865e-03\n",
      " -2.0553112e-04  3.4691966e-03 -9.3897223e-04  8.3817719e-03\n",
      "  9.0107834e-03  6.5365066e-03 -7.1162102e-04  7.7104042e-03\n",
      " -8.5343346e-03  3.2071066e-03 -4.6379971e-03 -5.0889552e-03\n",
      "  3.5896183e-03  5.3703394e-03  7.7695143e-03 -5.7665063e-03\n",
      "  7.4333609e-03  6.6254963e-03 -3.7098003e-03 -8.7456414e-03\n",
      "  5.4374672e-03  6.5097557e-03 -7.8755023e-04 -6.7098560e-03\n",
      " -7.0859254e-03 -2.4970602e-03  5.1432536e-03 -3.6652375e-03\n",
      " -9.3700597e-03  3.8267397e-03  4.8844791e-03 -6.4285635e-03\n",
      "  1.2085581e-03 -2.0748770e-03  2.4403334e-05 -9.8835090e-03\n",
      "  2.6920044e-03 -4.7501065e-03  1.0876465e-03 -1.5762246e-03\n",
      "  2.1966731e-03 -7.8815762e-03 -2.7171839e-03  2.6631986e-03\n",
      "  5.3466819e-03 -2.3915148e-03 -9.5100943e-03  4.5058788e-03]\n",
      "Words similar to 'natural': [('tasks', 0.16072483360767365), ('word2vec', 0.15923379361629486), ('nlp', 0.13725273311138153), ('am', 0.12300863116979599), ('about', 0.08544958382844925)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Vector for 'natural':\", vector)\n",
    "print(\"Words similar to 'natural':\", similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
